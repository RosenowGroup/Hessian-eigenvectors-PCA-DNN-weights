{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure weights for mulitple epochs\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import train\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '-n', '--name',\n",
    "    type=str,\n",
    "    default=\"missing name\",\n",
    "    help=\"Name of network\",\n",
    "    dest=\"name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-s', '--seed',\n",
    "    type=float,\n",
    "    default=1,\n",
    "    help=\"Seed for randomness\",\n",
    "    dest=\"seed\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-l', '--layer',\n",
    "    type=str,\n",
    "    default=\"all\",\n",
    "    help=\"layer_name, give all for all layers\",\n",
    "    dest=\"layer\"\n",
    ")\n",
    "\n",
    "args=parser.parse_args()\n",
    "model_str=args.name\n",
    "layer_str=args.layer\n",
    "tf.random.set_seed(args.seed)\n",
    "\n",
    "exec(open(model_str+'/'+model_str+\".py\").read())\n",
    "model.load_weights(model_str+'/saved_model/'+model_str)\n",
    "model,weights=train.weight_measure(model,layer_str,10,final_learning_rate,x_train,y_train,batch_size) # for 5000 epochs use \"measure_epochs_only=True\"\n",
    "np.save(model_str+'/weights_'+layer_str,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the principal components\n",
    "import numpy as np\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '-n', '--name',\n",
    "    type=str,\n",
    "    default=\"missing name\",\n",
    "    help=\"Name of network\",\n",
    "    dest=\"name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-l', '--layer',\n",
    "    type=str,\n",
    "    default=\"all\",\n",
    "    help=\"layer_name, give all for all layers\",\n",
    "    dest=\"layer\"\n",
    ")\n",
    "args=parser.parse_args()\n",
    "model_str=args.name\n",
    "layer_str=args.layer\n",
    "\n",
    "weights=np.load(model_str+'/weights_'+layer_str+'.npy')\n",
    "Cov=np.cov(weights.T)\n",
    "variance,pcomp_cov=np.linalg.eigh(Cov)\n",
    "pcomp=pcomp_cov.T[::-1]\n",
    "np.save(model_str+'/pc_'+layer_str,pcomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute theta\n",
    "import numpy as np\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '-n', '--name',\n",
    "    type=str,\n",
    "    default=\"missing name\",\n",
    "    help=\"Name of network\",\n",
    "    dest=\"name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-l', '--layer',\n",
    "    type=str,\n",
    "    default=\"all\",\n",
    "    help=\"layer_name, give all for all layers\",\n",
    "    dest=\"layer\"\n",
    ")\n",
    "args=parser.parse_args()\n",
    "model_str=args.name\n",
    "layer_str=args.layer\n",
    "\n",
    "weights=np.load(model_str+'/weights_'+layer_str+'.npy')\n",
    "pcomp=np.load(model_str+'/pc_'+layer_str+'.npy')\n",
    "theta=np.tensordot(weights,pcomp,axes=[1,1])\n",
    "np.save(model_str+'/theta_'+layer_str,theta.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import analysis\n",
    "#plt.rcParams['text.usetex'] = False\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '-n', '--name',\n",
    "    type=str,\n",
    "    default=\"missing name\",\n",
    "    help=\"Name of network\",\n",
    "    dest=\"name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-l', '--layer',\n",
    "    type=str,\n",
    "    default=\"all\",\n",
    "    help=\"layer_name, all or layer of sv\",\n",
    "    dest=\"layer\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-m', '--mode',\n",
    "    type=str,\n",
    "    default=\"evh\",\n",
    "    help=\"type of input vectors, evh or pc\",\n",
    "    dest=\"mode\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '-lx', '--lyrdex',\n",
    "    type=int,\n",
    "    default=\"-1\",\n",
    "    help=\"layer index for singular values\",\n",
    "    dest=\"lx\"\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "model_str = args.name\n",
    "md_str = args.mode\n",
    "layer_index = args.lx\n",
    "layer_str = args.layer\n",
    "\n",
    "\n",
    "exec(open(model_str+'/'+model_str+\".py\").read())\n",
    "field = analysis.sv_field(model, model_str, layer_str, md_str='pc')\n",
    "if md_str == \"evh\":\n",
    "    if layer_str == \"all\":\n",
    "        np.save(model_str+'/field_'+str(layer_index), field)\n",
    "    else:\n",
    "        np.save(model_str+'/field_'+layer_str, field)\n",
    "else:\n",
    "    np.save(model_str+'/field_'+layer_str+'_'+md_str, field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracies and losses for different number of components added\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import analysis\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '-n', '--name',\n",
    "    type=str,\n",
    "    default=\"missing name\",\n",
    "    help=\"Name of network\",\n",
    "    dest=\"name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-l', '--layer',\n",
    "    type=str,\n",
    "    default=\"all\",\n",
    "    help=\"layer_name, give all for all layers\",\n",
    "    dest=\"layer\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-m', '--mode',\n",
    "    type=str,\n",
    "    default=\"evh\",\n",
    "    help=\"type of input vectors, evh or pc\",\n",
    "    dest=\"mode\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-a', '--abs',\n",
    "    type=bool,\n",
    "    default=True,\n",
    "    help=\"whether eigenvalues should be added in order of magnitude or sign\",\n",
    "    dest=\"abs\"\n",
    ")\n",
    "\n",
    "args=parser.parse_args()\n",
    "model_str=args.name\n",
    "md_str=args.mode\n",
    "layer_str=args.layer\n",
    "\n",
    "\n",
    "exec(open(model_str+'/'+model_str+\".py\").read())\n",
    "model.load_weights(model_str+'/saved_model/'+model_str)\n",
    "if md_str==\"evh\":\n",
    "        if args.abs:\n",
    "            sort_list=np.argsort(np.abs(np.load(model_str+'/ewh_'+layer_str+'.npy')))\n",
    "            print(sort_list)\n",
    "            print(np.take(np.load(model_str+'/ewh_'+layer_str+'.npy'),sort_list,axis=0)[::-1])\n",
    "            vec=np.take(np.load(model_str+'/evh_'+layer_str+'.npy'),sort_list,axis=0)[::-1]\n",
    "        else:\n",
    "            vec=np.load(model_str+'/evh_'+layer_str+'.npy')\n",
    "else:\n",
    "    vec=np.load(model_str+'/'+md_str+'.npy')\n",
    "accs=analysis.acc_components(model,layer_str,vec,x_train,y_train,x_test,y_test)\n",
    "if args.abs:\n",
    "    np.save(model_str+'/accabs_'+layer_str+md_str,accs)\n",
    "else:\n",
    "    np.save(model_str+'/acc_'+layer_str+md_str,accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import time\n",
    "#plt.rcParams['text.usetex'] = False\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(\n",
    "    prog='train MLP256',\n",
    "    description='train MLP256',\n",
    "    epilog='Based on the rmt package.')\n",
    "parser.add_argument(\n",
    "    '-n', '--name',\n",
    "    type=str,\n",
    "    default=\"missing name\",\n",
    "    help=\"Name of network\",\n",
    "    dest=\"name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-r', '--reg',\n",
    "    type=float,\n",
    "    default=0,\n",
    "    help=\"Regularization\",\n",
    "    dest=\"reg\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-l', '--layer',\n",
    "    type=str,\n",
    "    default=\"all\",\n",
    "    help=\"layer_name, give all for all layers\",\n",
    "    dest=\"layer\"\n",
    ")\n",
    "\n",
    "args=parser.parse_args()\n",
    "model_str=args.name\n",
    "layer_str=args.layer\n",
    "if args.reg!=0:\n",
    "  reg=tf.keras.regularizers.l2(args.reg)\n",
    "else:\n",
    "  reg=None\n",
    "\n",
    "\n",
    "exec(open(model_str+'/'+model_str+\".py\").read())\n",
    "model.load_weights(model_str+'/saved_model/'+model_str)\n",
    "layer_name=[]\n",
    "b_list=[]\n",
    "if layer_str==\"all\":\n",
    "  for layer in model.trainable_weights:\n",
    "    if len(layer.shape)==1:\n",
    "      b_list.append(np.array(layer))\n",
    "    if len(layer.shape)>0:\n",
    "      layer_name.append(np.array(layer))\n",
    "else:\n",
    "  for layer in getattr(model,layer_str).get_weights():\n",
    "    if len(layer.shape)==1:\n",
    "      b_list.append(layer)\n",
    "    if len(layer.shape)>1:\n",
    "      layer_name.append(layer)\n",
    "print(len(layer_name))\n",
    "print(len(b_list))\n",
    "#computes the number of parameters of the layer\n",
    "layer_size = 0\n",
    "for layer in layer_name:\n",
    "    layer_size += np.prod(layer.shape)\n",
    "#conserve the numpy datatype\n",
    "layer_dtype = np.float32\n",
    "layer_lengths=[]\n",
    "for layer in layer_name:\n",
    "  layer_lengths.append(tf.math.reduce_prod(layer.shape))\n",
    "batch_size=12500\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(batch_size)\n",
    "\"\"\"if layer_str==\"all\":\n",
    "  evh=np.load(model_str+'/ev_sparse'+'.npy').T[::-1]\"\"\"\n",
    "\n",
    "#evh=np.load(model_str+'/evh_'+layer_str+'.npy')\n",
    "#ewh=np.load(model_str+'/ewh_'+layer_str+'.npy')\n",
    "\n",
    "@tf.function\n",
    "def flatten(grad):\n",
    "    temp = tf.TensorArray(tf.float32, size=0, dynamic_size=True, infer_shape=False)\n",
    "    for gr in grad:\n",
    "        temp = temp.write(temp.size(), tf.reshape(gr, (tf.math.reduce_prod(tf.shape(gr)), )))\n",
    "    return temp.concat()\n",
    "weights_0=flatten(layer_name)\n",
    "\n",
    "@tf.function\n",
    "def repack(weights):\n",
    "    weight_split=tf.split(weights,layer_lengths)\n",
    "    templer = []\n",
    "    for layer,weights_set in zip(layer_name,weight_split):\n",
    "        templ = tf.reshape(weights_set,layer.shape)\n",
    "        templer.append(templ)\n",
    "    return templer\n",
    "\n",
    "if layer_str==\"all\":\n",
    "  def set_weights(weights):\n",
    "    templ=repack(weights)\n",
    "    \"\"\"for i in range(len(layer_name)):\n",
    "      model.layers[i+1].set_weights([templ[i],b_list[i]])\"\"\"\n",
    "    k=0\n",
    "    for i in range(len(model.layers)):\n",
    "        if len(model.layers[i].get_weights())==2:\n",
    "            model.layers[i].set_weights([templ[k],templ[k+1]])\n",
    "            k+=2\n",
    "        elif len(model.layers[i].get_weights())==1:\n",
    "            model.layers[i].set_weights([templ[k]])\n",
    "            k+=1\n",
    "else:\n",
    "  def set_weights(weights):\n",
    "    templ=repack(weights)\n",
    "    getattr(model,layer_str).set_weights([templ[0],b_list[0]])\n",
    "\n",
    "\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "@tf.function\n",
    "def calc_acc(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    test_accuracy(labels, predictions)\n",
    "    loss=loss_object(labels, predictions)\n",
    "    loss+=sum(model.losses)\n",
    "    train_loss(loss)\n",
    "#weights_prod=np.tensordot(weights_0,evh,axes=(0,1))\n",
    "#max_entry=np.argmax(weights_prod)\n",
    "step_range=0#evh.shape[0]\n",
    "\n",
    "step_array=np.concatenate(([0],[1],[2]))\n",
    "shift_step=0.01\n",
    "#for i in step_array:\n",
    "#shift=weights_prod[max_entry]\n",
    "epsilons=np.arange(-2,4,shift_step)\n",
    "trainacc=np.empty(epsilons.shape[0])\n",
    "trainloss=np.empty(epsilons.shape[0])\n",
    "testacc=np.empty(epsilons.shape[0])\n",
    "index=0\n",
    "for images, labels in train_ds:\n",
    "      calc_acc(images, labels)\n",
    "loss_0=train_loss.result()\n",
    "train_loss.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "incl_vecs=np.arange(-step_range,step_range+1,1)\n",
    "print(incl_vecs)\n",
    "for epsilon in epsilons:\n",
    "    weights_set=weights_0#-np.tensordot(evh[incl_vecs],weights_prod[incl_vecs],axes=[0,0])\n",
    "    \n",
    "    for i in incl_vecs:\n",
    "        weights_set+=(epsilon)*weights_0\n",
    "    set_weights(weights_set)\n",
    "    for images, labels in train_ds:\n",
    "      calc_acc(images, labels)\n",
    "    trainacc[index]=test_accuracy.result()\n",
    "    trainloss[index]=train_loss.result()\n",
    "    train_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    calc_acc(x_test,y_test)\n",
    "    testacc[index]=test_accuracy.result()\n",
    "    train_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    index+=1\n",
    "np.save(model_str+'/loss_scaling',[epsilons,trainacc,trainloss,testacc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss landscape\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(\n",
    "    prog='train MLP256',\n",
    "    description='train MLP256',\n",
    "    epilog='Based on the rmt package.')\n",
    "parser.add_argument(\n",
    "    '-n', '--name',\n",
    "    type=str,\n",
    "    default=\"missing name\",\n",
    "    help=\"Name of network\",\n",
    "    dest=\"name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-l', '--layer',\n",
    "    type=str,\n",
    "    default=\"all\",\n",
    "    help=\"layer_name, give all for all layers\",\n",
    "    dest=\"layer\"\n",
    ")\n",
    "\n",
    "args=parser.parse_args()\n",
    "model_str=args.name\n",
    "layer_str=args.layer\n",
    "if args.reg!=0:\n",
    "  reg=tf.keras.regularizers.l2(args.reg)\n",
    "else:\n",
    "  reg=None\n",
    "\n",
    "\n",
    "exec(open(model_str+'/'+model_str+\".py\").read())\n",
    "model.load_weights(model_str+'/saved_model/'+model_str)\n",
    "layer_name=[]\n",
    "b_list=[]\n",
    "if layer_str==\"all\":\n",
    "  for layer in model.trainable_weights:\n",
    "    if len(layer.shape)==1:\n",
    "      b_list.append(np.array(layer))\n",
    "    if len(layer.shape)>0:\n",
    "      layer_name.append(np.array(layer))\n",
    "else:\n",
    "  for layer in getattr(model,layer_str).get_weights():\n",
    "    if len(layer.shape)==1:\n",
    "      b_list.append(layer)\n",
    "    if len(layer.shape)>1:\n",
    "      layer_name.append(layer)\n",
    "print(len(layer_name))\n",
    "print(len(b_list))\n",
    "#computes the number of parameters of the layer\n",
    "layer_size = 0\n",
    "for layer in layer_name:\n",
    "    layer_size += np.prod(layer.shape)\n",
    "#conserve the numpy datatype\n",
    "layer_dtype = np.float32\n",
    "layer_lengths=[]\n",
    "for layer in layer_name:\n",
    "  layer_lengths.append(tf.math.reduce_prod(layer.shape))\n",
    "batch_size=12500\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(batch_size)\n",
    "\"\"\"if layer_str==\"all\":\n",
    "  evh=np.load(model_str+'/ev_sparse'+'.npy').T[::-1]\"\"\"\n",
    "\n",
    "evh=np.load(model_str+'/evh_'+layer_str+'.npy')\n",
    "ewh=np.load(model_str+'/ewh_'+layer_str+'.npy')\n",
    "\n",
    "@tf.function\n",
    "def flatten(grad):\n",
    "    temp = tf.TensorArray(tf.float32, size=0, dynamic_size=True, infer_shape=False)\n",
    "    for gr in grad:\n",
    "        temp = temp.write(temp.size(), tf.reshape(gr, (tf.math.reduce_prod(tf.shape(gr)), )))\n",
    "    return temp.concat()\n",
    "weights_0=flatten(layer_name)\n",
    "\n",
    "@tf.function\n",
    "def repack(weights):\n",
    "    weight_split=tf.split(weights,layer_lengths)\n",
    "    templer = []\n",
    "    for layer,weights_set in zip(layer_name,weight_split):\n",
    "        templ = tf.reshape(weights_set,layer.shape)\n",
    "        templer.append(templ)\n",
    "    return templer\n",
    "\n",
    "if layer_str==\"all\":\n",
    "  def set_weights(weights):\n",
    "    templ=repack(weights)\n",
    "    \"\"\"for i in range(len(layer_name)):\n",
    "      model.layers[i+1].set_weights([templ[i],b_list[i]])\"\"\"\n",
    "    k=0\n",
    "    for i in range(len(model.layers)):\n",
    "        if len(model.layers[i].get_weights())==2:\n",
    "            model.layers[i].set_weights([templ[k],templ[k+1]])\n",
    "            k+=2\n",
    "        elif len(model.layers[i].get_weights())==1:\n",
    "            model.layers[i].set_weights([templ[k]])\n",
    "            k+=1\n",
    "else:\n",
    "  def set_weights(weights):\n",
    "    templ=repack(weights)\n",
    "    getattr(model,layer_str).set_weights([templ[0],b_list[0]])\n",
    "\n",
    "\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "@tf.function\n",
    "def calc_acc(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    test_accuracy(labels, predictions)\n",
    "    loss=loss_object(labels, predictions)\n",
    "    loss+=sum(model.losses)\n",
    "    train_loss(loss)\n",
    "weights_prod=np.tensordot(weights_0,evh,axes=(0,1))\n",
    "max_entry=np.argmax(weights_prod)\n",
    "step_range=0#evh.shape[0]\n",
    "\n",
    "step_array=np.concatenate(([0],[max_entry]))\n",
    "print(step_array)\n",
    "shift_step=0.01\n",
    "for i in step_array:\n",
    "    shift=weights_prod[i]\n",
    "    epsilons=np.arange(shift-weights_prod[max_entry]-shift_step,2*shift_step+shift+weights_prod[max_entry],shift_step)\n",
    "    trainacc=np.empty(epsilons.shape[0])\n",
    "    trainloss=np.empty(epsilons.shape[0])\n",
    "    testacc=np.empty(epsilons.shape[0])\n",
    "    index=0\n",
    "    for images, labels in train_ds:\n",
    "          calc_acc(images, labels)\n",
    "    loss_0=train_loss.result()\n",
    "    train_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    incl_vecs=np.arange(-step_range+i,step_range+i+1,1)\n",
    "    print(incl_vecs)\n",
    "    for epsilon in epsilons:\n",
    "        weights_set=weights_0#-np.tensordot(evh[incl_vecs],weights_prod[incl_vecs],axes=[0,0])\n",
    "        \n",
    "        for i in incl_vecs:\n",
    "            weights_set+=(epsilon-weights_prod[i])*evh[i]\n",
    "        set_weights(weights_set)\n",
    "        for images, labels in train_ds:\n",
    "          calc_acc(images, labels)\n",
    "        trainacc[index]=test_accuracy.result()\n",
    "        trainloss[index]=train_loss.result()\n",
    "        train_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "        calc_acc(x_test,y_test)\n",
    "        testacc[index]=test_accuracy.result()\n",
    "        train_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "        index+=1\n",
    "    np.save(model_str+'/eps/evh_eps_'+str(i),[epsilons,trainacc,trainloss,testacc])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
